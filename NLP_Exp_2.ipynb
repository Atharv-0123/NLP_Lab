{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R056Racd22oP"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    # Tokenize the text using regular expression\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "aCLpip0x3BHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_tokens(tokens):\n",
        "    # Filter out stopwords or non-alphabetic tokens\n",
        "    stopwords = set(['the', 'a', 'an', 'in', 'on', 'at', 'to', 'of', 'and', 'or'])\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords and token.isalpha()]\n",
        "    return filtered_tokens\n"
      ],
      "metadata": {
        "id": "SqiEsXxA3Euu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_script(tokens):\n",
        "    # Check if the text contains specific words or phrases\n",
        "    forbidden_words = set(['bad', 'forbidden', 'unwanted'])\n",
        "    contains_forbidden = any(word in tokens for word in forbidden_words)\n",
        "    return not contains_forbidden\n"
      ],
      "metadata": {
        "id": "SDcfWQlM3HMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"This is an example text with some bad and unwanted words.\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = tokenize(text)\n",
        "\n",
        "# Filter tokens\n",
        "filtered_tokens = filter_tokens(tokens)\n",
        "\n",
        "# Validate script\n",
        "valid_script = validate_script(filtered_tokens)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"\\nTokenized Text:\", tokens)\n",
        "print(\"\\nFiltered Tokens:\", filtered_tokens)\n",
        "\n",
        "if valid_script:\n",
        "    print(\"\\nScript is valid.\")\n",
        "else:\n",
        "    print(\"\\nScript contains forbidden words.\")\n"
      ],
      "metadata": {
        "id": "8IgwBLNQ3J41",
        "outputId": "de082ecb-872a-4569-9251-b0e05cb0224d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: This is an example text with some bad and unwanted words.\n",
            "\n",
            "Tokenized Text: ['this', 'is', 'an', 'example', 'text', 'with', 'some', 'bad', 'and', 'unwanted', 'words']\n",
            "\n",
            "Filtered Tokens: ['this', 'is', 'example', 'text', 'with', 'some', 'bad', 'unwanted', 'words']\n",
            "\n",
            "Script contains forbidden words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVQggDVk3Lsy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}